<!DOCTYPE html>
<html lang="ko">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>문서 파싱의 구조 보존이 RAG 파이프라인에 미치는 영향 연구 | Hyeongseob's Note</title>

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css" rel="stylesheet">

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">

  <!-- Prism.js -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css">

  <!-- Report Styles -->
  <link rel="stylesheet" href="report.css">
</head>

<body>
  <header class="report-header">
    <div class="report-header__inner">
      <a href="../hub/" class="report-header__brand">Hyeongseob's Note</a>
      <nav class="report-header__nav">
        <a href="./" class="report-header__link">Tech Report</a>
        <a href="https://harrison-kim.tistory.com/" target="_blank" rel="noopener noreferrer"
          class="report-header__link">Blog</a>
        <a href="https://www.wigtn.com/" target="_blank" rel="noopener noreferrer"
          class="report-header__link">WIGTN</a>
        <a href="../" class="report-header__link report-header__link--cta">Portfolio</a>
      </nav>
    </div>
  </header>

  <main class="report-main">
    <article>
      <header>
        <a href="../hub/" class="report-article__back">&larr; Hub</a>
        <h1 class="report-article__title">문서 파싱의 구조 보존이 RAG 파이프라인에 미치는 영향 연구</h1>
        <div class="report-article__meta">
          <div class="report-article__meta-left">
            <time datetime="2026-01-28">2026.01.28</time>
            <span class="report-article__tags">
              <span class="report-article__tag">VLM</span>
              <span class="report-article__tag">RAG</span>
              <span class="report-article__tag">Document Parsing</span>
              <span class="report-article__tag">OCR</span>
            </span>
          </div>
          <a href="https://github.com/Hyeongseob91/test-vlm-document-parsing" target="_blank" rel="noopener noreferrer"
            class="report-article__github"
            title="GitHub Repository" aria-label="GitHub Repository">
            <svg viewBox="0 0 24 24" width="18" height="18" fill="currentColor">
              <path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/>
            </svg>
          </a>
        </div>
      </header>

      <div class="report-content">

        <!-- Abstract -->
        <h2>Abstract</h2>
        <p>
          AI 파이프라인의 첫 단계인 Document Parsing에서 구조 정보가 손실되면,
          이후 어떤 청킹 전략이나 검색 알고리즘을 적용해도 품질의 한계에 부딪힙니다.
          이 실험에서는 이 가설을 정량적으로 검증하기 위해,
          표 안의 표, 이미지 기반 테이블, 다단 레이아웃 등 복잡한 구조를 포함하는 문서를 대상으로
          기존 텍스트 추출/OCR 방식(PyMuPDF, RapidOCR)과 VLM(Qwen3-VL) 기반 구조화 파싱을
          <strong>Lexical Accuracy</strong>, <strong>Structural Integrity</strong>,
          <strong>Downstream Chunking Quality</strong>, <strong>Retrieval Impact</strong>
          네 가지 차원에서 비교 분석했습니다.
        </p>

        <blockquote>
          <p>
            기존 OCR 파서(PyMuPDF, RapidOCR)는 텍스트 추출 자체는 잘 수행합니다(CER <strong>40–51%</strong>).
            그러나 추출된 결과에 구조 정보가 전혀 없어 Structure F1은 <strong>0%</strong>입니다.
            여기에 VLM 기반 2-Stage Parsing을 적용하면, 텍스트 추출 품질은 유지하면서
            Structure Precision <strong>72.41%</strong>, Recall <strong>87.50%</strong>
            (F1 <strong>79.25%</strong>)을 달성합니다.
            다만 CER이 소폭 증가하는 트레이드오프가 존재하며,
            이는 Prompt Engineering(v1 → v2)을 통해 개선했습니다.
            자세한 실험 과정과 결과는 아래에서 살펴보겠습니다.
          </p>
        </blockquote>

        <!-- Motivation -->
        <h2>1. Motivation</h2>

        <h3>1.1 문제 인식</h3>
        <p>
          고객사의 RAG 파이프라인을 구축하면서 반복적으로 마주한 문제가 있습니다.
          고객사 문서에는 <strong>표 안의 표</strong>, <strong>이미지 기반 테이블</strong>, <strong>다단 레이아웃</strong> 등
          복잡한 구조가 포함되어 있었습니다. 이런 문서를 기존 OCR 파이프라인으로 추출하면
          <strong>테이블이 중간에 잘리거나</strong>, <strong>다단 레이아웃의 읽기 순서가 뒤섞이는</strong> 현상이 발생했습니다.
          청킹 전략을 바꾸고, 임베딩 모델을 교체하고, Reranker를 추가해도 근본적인 한계가 해소되지 않았습니다.
        </p>

        <p>
          결국 문제의 원인은 파이프라인의 첫 단계인 <strong>Data Parsing</strong>에 있었습니다.
          구조가 손실된 상태로 넘어온 텍스트는, 이후 어떤 고도화를 적용해도 복원이 불가능합니다.
        </p>

        <p>예를 들어, 아래와 같은 2단 학술 논문을 PyMuPDF로 추출하면 어떻게 될까요?</p>

        <pre><code class="language-text"># 기대하는 결과
1. Introduction
   문단 1 내용...
   문단 2 내용...

# 실제 결과 (PyMuPDF)
1. Introduction    문단 1 첫 줄    문단 2 첫 줄
문단 1 두번째 줄    문단 2 두번째 줄...</code></pre>

        <p>
          이러한 구조적 손실은 <strong>청킹 품질 저하</strong> → <strong>검색 정확도 하락</strong>으로 직결됩니다.
          그렇다면 파싱 단계에서 구조를 보존하면 이 문제를 해결할 수 있을까요?
          VLM 기반 구조화 파싱을 통해 정량적으로 검증해 보겠습니다.
        </p>

        <h3>1.2 Research Questions</h3>
        <p>이 실험에서는 네 가지 핵심 질문에 답하고자 합니다:</p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>RQ</th>
              <th>역할</th>
              <th>질문</th>
              <th>측정 지표</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>RQ1</strong></td>
              <td>전제조건</td>
              <td>VLM 기반 파싱이 텍스트 정확도를 유지하는가?</td>
              <td>CER, WER</td>
            </tr>
            <tr>
              <td><strong>RQ2</strong></td>
              <td>핵심 가설</td>
              <td>VLM 기반 파싱이 문서 구조를 더 잘 보존하는가?</td>
              <td>Structure F1</td>
            </tr>
            <tr>
              <td><strong>RQ3</strong></td>
              <td>효과 검증</td>
              <td>구조 보존이 다운스트림(청킹) 품질을 향상시키는가?</td>
              <td>BC Score, CS</td>
            </tr>
            <tr>
              <td><strong>RQ4</strong></td>
              <td>End-to-End</td>
              <td>개선된 청킹이 실제 검색 정밀도를 높이는가?</td>
              <td>Hit Rate@k, MRR</td>
            </tr>
          </tbody>
        </table></div>

        <h3>1.3 "구조화된 데이터" 정의</h3>
        <p>이 실험에서 "구조화된 데이터"란 다음을 의미합니다:</p>
        <ul>
          <li><strong>마크다운 헤딩</strong>: 문서의 계층적 섹션 구조 (#, ##, ###)</li>
          <li><strong>테이블</strong>: 행/열로 구성된 표 형식 데이터 (| col1 | col2 |)</li>
          <li><strong>리스트</strong>: 순서 있는/없는 목록 (1., -, *)</li>
          <li><strong>읽기 순서</strong>: 다단 레이아웃에서 올바른 텍스트 흐름</li>
        </ul>

        <h3>1.4 Core Hypothesis</h3>
        <blockquote>
          <p>
            "파이프라인의 첫 단계(Data Parsing)에서 구조를 보존하면,
            동일한 downstream 처리(청킹, 임베딩, 검색)로도 더 높은 품질을 달성할 수 있다."
          </p>
        </blockquote>

        <!-- Related Work -->
        <h2>2. Related Work</h2>

        <h3>2.1 Traditional Text Extraction &amp; OCR Approaches</h3>
        <p>
          전통적인 텍스트 추출/OCR 파이프라인(Tesseract, RapidOCR, PyMuPDF)은
          문자 인식에는 우수하지만 <strong>레이아웃 이해 능력이 제한적</strong>입니다.
          특히 다단 레이아웃, 표, 중첩 구조에서 읽기 순서가 뒤섞이는 문제가 발생합니다.
        </p>

        <h3>2.2 Layout-Aware Models</h3>
        <p>
          LayoutLM 시리즈(v1, v2, v3)는 텍스트와 레이아웃 정보를 함께 학습하여
          문서 이해 성능을 향상시켰습니다. 하지만 이러한 모델들에는 한계가 있습니다:
        </p>
        <ul>
          <li>사전 학습된 레이아웃 패턴에 의존</li>
          <li>새로운 문서 형식에 대한 일반화 제한</li>
          <li>구조화된 출력(마크다운) 생성에 최적화되지 않음</li>
        </ul>

        <h3>2.3 Vision-Language Models</h3>
        <p>
          Qwen-VL, GPT-4V, Claude 3 등 최신 VLM은 이미지를 직접 이해하고
          구조화된 텍스트를 생성할 수 있습니다. 이 실험에서는 Qwen3-VL-2B-Instruct를 사용했습니다:
        </p>
        <ul>
          <li><strong>End-to-End 구조화</strong>: 이미지 → 마크다운 직접 변환</li>
          <li><strong>Zero-shot 일반화</strong>: 학습하지 않은 문서 형식도 처리 가능</li>
          <li><strong>다국어 지원</strong>: 한국어, 영어 등 다양한 언어 처리</li>
        </ul>

        <h3>2.4 Prompt Engineering Evolution</h3>
        <p>
          VLM 기반 문서 파싱에서 프롬프트 설계는 결과 품질에 큰 영향을 미칩니다.
          이 실험에서는 "Extraction" 패러다임에서 "Transcription" 패러다임으로 전환하여
          Hallucination을 최소화하는 전략을 도입했습니다.
        </p>

        <h3>2.5 Semantic Chunking & Evaluation</h3>
        <p>
          RAG 파이프라인에서 청킹 품질 평가는 아직 표준화되지 않았습니다.
          이 실험에서는 다음 메트릭을 활용합니다:
        </p>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>메트릭</th>
              <th>설명</th>
              <th>출처</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Structure F1</td>
              <td>구조 요소 검출 정확도</td>
              <td>본 연구 정의</td>
            </tr>
            <tr>
              <td>Boundary Clarity (BC)</td>
              <td>인접 청크 간 의미적 분리도</td>
              <td>Zhao et al. (2025), MoC</td>
            </tr>
            <tr>
              <td>Chunk Stickiness (CS)</td>
              <td>청크 내 문장 결속도 (Structural Entropy)</td>
              <td>Zhao et al. (2025), MoC</td>
            </tr>
          </tbody>
        </table></div>

        <!-- Methodology -->
        <h2>3. Methodology</h2>

        <h3>3.1 평가 프레임워크</h3>
        <p>4단계 평가 프레임워크를 설계하여 문서 파싱 품질을 다각도로 측정했습니다:</p>

        <pre><code class="language-text">┌──────────────────────────────────────────────────────────────────────────────────┐
│                           EVALUATION FRAMEWORK                                   │
├──────────────────────────────────────────────────────────────────────────────────┤
│                                                                                  │
│  ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────┐                     │
│  │  RQ1     │   │  RQ2     │   │  RQ3     │   │  RQ4     │                     │
│  │ Lexical  │ → │Structural│ → │Downstream│ → │Retrieval │                     │
│  │ Accuracy │   │Integrity │   │ Chunking │   │ Impact   │                     │
│  └──────────┘   └──────────┘   └──────────┘   └──────────┘                     │
│       │              │              │              │                            │
│       ▼              ▼              ▼              ▼                            │
│  ┌──────────┐   ┌──────────┐   ┌──────────┐   ┌──────────┐                    │
│  │ CER, WER │   │    F1    │   │  BC, CS  │   │Hit Rate, │                    │
│  │          │   │          │   │          │   │   MRR    │                    │
│  └──────────┘   └──────────┘   └──────────┘   └──────────┘                    │
│                                                                                  │
└──────────────────────────────────────────────────────────────────────────────────┘</code></pre>

        <h3>3.2 파서 아키텍처</h3>
        <p>4가지 파서 조합을 비교 평가했습니다:</p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Parser</th>
              <th>Stage 1</th>
              <th>Stage 2</th>
              <th>적합 문서 유형</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Text-Baseline</strong></td>
              <td>PyMuPDF</td>
              <td>-</td>
              <td>디지털 PDF (텍스트 레이어 존재)</td>
            </tr>
            <tr>
              <td><strong>Image-Baseline</strong></td>
              <td>RapidOCR</td>
              <td>-</td>
              <td>스캔 PDF, 이미지 기반 문서</td>
            </tr>
            <tr>
              <td><strong>Text-Advanced</strong></td>
              <td>PyMuPDF</td>
              <td>VLM 구조화</td>
              <td>디지털 PDF + 구조 추출</td>
            </tr>
            <tr>
              <td><strong>Image-Advanced</strong></td>
              <td>RapidOCR</td>
              <td>VLM 구조화</td>
              <td>스캔 PDF + 구조 추출</td>
            </tr>
          </tbody>
        </table></div>

        <h3>3.3 PDF Type 분류</h3>
        <p>
          같은 "PDF"라도 내부 구조에 따라 파서 동작이 크게 달라집니다.
          이 실험에서는 PDF를 다음 4가지 Type으로 분류하여 테스트 커버리지를 관리합니다.
        </p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>PDF Type</th>
              <th>특성</th>
              <th>파서 동작 차이</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Digital PDF</strong></td>
              <td>텍스트 레이어 존재 (arXiv, Word→PDF)</td>
              <td>PyMuPDF가 텍스트 직접 추출, OCR 불필요</td>
            </tr>
            <tr>
              <td><strong>Scanned PDF</strong></td>
              <td>이미지만 존재 (스캔 문서)</td>
              <td>OCR 필수, 스캔 품질에 따라 정확도 편차 큼</td>
            </tr>
            <tr>
              <td><strong>Hybrid PDF</strong></td>
              <td>일부 페이지만 스캔</td>
              <td>페이지별 처리 전략 필요</td>
            </tr>
            <tr>
              <td><strong>Image-heavy PDF</strong></td>
              <td>텍스트는 있으나 표/차트가 이미지로 삽입</td>
              <td>텍스트는 추출되나 표 구조 손실</td>
            </tr>
          </tbody>
        </table></div>

        <p>
          분류 기준으로 80%를 사용하는 것은 휴리스틱입니다.
          100%를 사용하면 단 1페이지라도 다른 유형이면 분류가 실패하므로,
          약간의 여유를 두어 주요 특성을 반영합니다.
        </p>

        <pre><code class="language-python">import fitz  # PyMuPDF

def classify_pdf(pdf_path):
    """PDF를 4가지 Type으로 분류한다."""
    doc = fitz.open(pdf_path)
    text_pages = 0
    image_pages = 0
    image_heavy_pages = 0

    for page in doc:
        text = page.get_text().strip()
        images = page.get_images()

        # 텍스트가 있고 이미지가 많으면 Image-heavy
        if len(text) > 100 and len(images) > 3:
            image_heavy_pages += 1
        elif len(text) > 100:
            text_pages += 1
        elif images:
            image_pages += 1

    total = len(doc)
    if image_heavy_pages > total * 0.5:
        return "Image-heavy PDF"
    elif text_pages > total * 0.8:
        return "Digital PDF"
    elif image_pages > total * 0.8:
        return "Scanned PDF"
    else:
        return "Hybrid PDF"</code></pre>

        <h3>3.4 VLM 프롬프트 전략</h3>
        <p>
          본 연구에서 가장 결정적인 성능 차이를 만든 요인은 모델이나 파이프라인 구조가 아닌
          <strong>프롬프트 설계</strong>였습니다. 동일한 Qwen3-VL-2B 모델에서 프롬프트만 변경하여
          Structure F1이 0%에서 77~79%로 개선되었습니다.
          이 절에서는 v1에서 v2로의 진화 과정과 그 근거를 상세히 기술합니다.
        </p>

        <h4>3.4.1 Prompt v1: Extraction Expert (Baseline)</h4>
        <p>
          초기 프롬프트는 VLM을 "전문가(expert)"로 프레이밍하여 문서의 모든 정보를
          추출하도록 지시했습니다.
        </p>

        <pre><code class="language-python"># Prompt v1 — Extraction Expert
PROMPT_V1 = """
You are an expert document extraction assistant. Your task is to extract
all information from the given document image and present it in a clear,
organized markdown format.

Please extract:
1. All text content
2. Tables (preserve structure)
3. Lists and enumerations
4. Headers and section titles

Format the output as clean markdown.
"""</code></pre>

        <p>
          이 프롬프트로 두 Advanced 파서 모두 <strong>Structure F1 = 0%</strong>를 기록하며 완전히 실패했습니다.
          "expert"라는 역할 부여와 "extract... present... organized"라는 지시가 모델에게
          <em>해석과 재구성</em>을 유도했고, 2B 파라미터의 소형 모델은 이를 hallucination으로 수행하여
          원본에 없는 구조를 만들어내거나 기존 구조를 무시했습니다.
        </p>

        <h4>3.4.2 Prompt v2: Transcription Engine (Selected)</h4>
        <p>
          v1의 실패를 분석한 뒤, 근본적으로 다른 접근을 설계했습니다.
          핵심 전환은 <strong>"추출(Extraction)"에서 "전사(Transcription)"로의 패러다임 변경</strong>입니다.
        </p>

        <pre><code class="language-python"># Prompt v2 — Transcription Engine (Selected)
PROMPT_V2 = """
You are a document transcription engine. Your sole purpose is to convert
the given image into markdown text format. You MUST only transcribe what
is actually visible in the image - do not add any additional information,
explanations, or content that is not present in the original document.

Rules:
1. Transcribe ALL visible text exactly as shown
2. Use markdown formatting to preserve structure (headers, lists, tables)
3. For tables, use markdown table syntax with proper alignment
4. Preserve the original language (Korean, English, etc.)
5. Do NOT add explanations, summaries, or interpretations
6. Do NOT add information that is not visible in the image
7. If text is unclear, indicate with [unclear] rather than guessing
8. Maintain original paragraph breaks and spacing intent
"""</code></pre>

        <p>
          VLM이 출력한 텍스트를 마크다운 구조로 변환하는 2단계(Text Structuring)에서는
          <strong>번호-마크다운 레벨 간의 명시적 매핑 규칙</strong>을 별도로 정의했습니다.
        </p>

        <pre><code class="language-python"># Text Structuring Prompt — 명시적 구조 매핑 규칙
SYSTEM_PROMPT = """You are a Markdown formatting expert. Your task is to
convert plain text into well-structured Markdown format.

CRITICAL RULES - You MUST follow these:
1. ALWAYS use # symbols for headings. This is mandatory.
2. Document title → # Title
3. Section numbers like "1 Introduction" or "1. Introduction" → ## 1. Introduction
4. Subsections like "3.1 Method" → ### 3.1 Method
5. Sub-subsections like "3.1.1 Details" → #### 3.1.1 Details
6. Tables with aligned columns → Markdown table with | separators
7. Bullet points → - item
8. Numbered lists → 1. item

NEVER output plain text headings without # symbols."""</code></pre>

        <h4>3.4.3 v1 vs v2 설계 차이 분석</h4>
        <p>
          두 프롬프트의 핵심 차이를 5개 차원에서 비교합니다.
          각 차원이 2B 소형 모델의 행동에 어떤 영향을 미쳤는지 분석합니다.
        </p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>설계 차원</th>
              <th>v1 (Extraction Expert)</th>
              <th>v2 (Transcription Engine)</th>
              <th>영향</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Role Framing</strong></td>
              <td>"expert extraction <em>assistant</em>"</td>
              <td>"transcription <em>engine</em>"</td>
              <td>"assistant"는 해석을 유도, "engine"은 기계적 전사를 유도</td>
            </tr>
            <tr>
              <td><strong>Task Definition</strong></td>
              <td>"Extract all information and <em>present</em> it"</td>
              <td>"You <strong>MUST</strong> only transcribe what is <em>actually visible</em>"</td>
              <td>"present"는 재구성 허용, "MUST only"는 범위를 제한</td>
            </tr>
            <tr>
              <td><strong>Explicit Negation</strong></td>
              <td>(없음)</td>
              <td>"Do <strong>NOT</strong> add explanations, summaries, or interpretations"</td>
              <td>소형 모델은 암묵적 기대를 추론하지 못하므로 명시적 금지가 필수</td>
            </tr>
            <tr>
              <td><strong>Uncertainty Handling</strong></td>
              <td>(없음 — 모델이 추측)</td>
              <td>"If text is unclear, indicate with <code>[unclear]</code> rather than guessing"</td>
              <td>hallucination의 주요 원인인 "추측"을 차단하는 안전장치</td>
            </tr>
            <tr>
              <td><strong>Structure Mapping</strong></td>
              <td>"Headers and section titles" (암묵적)</td>
              <td>"1 Introduction" → <code>## 1. Introduction</code><br>"3.1 Method" → <code>### 3.1 Method</code> (명시적)</td>
              <td>2B 모델의 제한된 추론 능력을 규칙 기반 매핑으로 보완</td>
            </tr>
          </tbody>
        </table></div>

        <h4>3.4.4 결과: Structure F1 변화</h4>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Prompt Version</th>
              <th>Text-Advanced</th>
              <th>Image-Advanced</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>v1 (Extraction Expert)</td>
              <td class="value-negative"><strong>0%</strong></td>
              <td class="value-negative"><strong>0%</strong></td>
            </tr>
            <tr>
              <td>v2 (Transcription Engine)</td>
              <td class="value-positive"><strong>79.25%</strong></td>
              <td class="value-positive"><strong>77.78%</strong></td>
            </tr>
            <tr>
              <td><strong>Delta</strong></td>
              <td><strong>+79.25pp</strong></td>
              <td><strong>+77.78pp</strong></td>
            </tr>
          </tbody>
        </table></div>

        <h4>3.4.5 Prompt Engineering Lessons</h4>
        <p>
          본 실험에서 도출한 소형 VLM(2B) 대상 프롬프트 설계 원칙을 정리합니다.
          이 원칙들은 대형 모델에서는 불필요할 수 있으나,
          자원 제약 환경에서 소형 모델을 활용할 때 실질적인 가이드라인이 됩니다.
        </p>
        <ol>
          <li>
            <strong>Explicit Negation</strong>:
            "Do NOT add"는 암묵적 기대보다 효과적입니다.
            소형 모델은 "clean markdown"이라는 지시에서 "불필요한 내용을 추가하지 말 것"을 추론하지 못합니다.
          </li>
          <li>
            <strong>Role Framing</strong>:
            "transcription engine"은 "expert assistant"보다 기계적·리터럴한 행동을 유도합니다.
            "expert"는 모델에게 판단과 해석의 여지를 부여하여 hallucination 위험을 높입니다.
          </li>
          <li>
            <strong>Uncertainty Handling</strong>:
            <code>[unclear]</code> 마커를 제공하면 모델이 불확실한 입력에 대해
            "추측" 대신 "표시"를 선택할 수 있는 경로가 생깁니다.
          </li>
          <li>
            <strong>Deterministic Mapping</strong>:
            "섹션 번호 → 마크다운 헤딩 레벨"과 같은 규칙 기반 매핑은
            소형 모델의 제한된 추론 능력을 보완하는 가장 효과적인 방법입니다.
          </li>
          <li>
            <strong>MUST/NEVER Keywords</strong>:
            강제 키워드(<code>MUST</code>, <code>NEVER</code>, <code>ALWAYS</code>)는
            소형 모델의 instruction following 성능을 향상시킵니다.
            v2 프롬프트에서 이 키워드들이 없으면 Structure F1이 현저히 낮아지는 것을 확인했습니다.
          </li>
        </ol>

        <blockquote>
          <p>
            <strong>핵심 교훈</strong>: 2B 파라미터 소형 모델에서는 프롬프트가 모델 선택만큼 중요한 변수입니다.
            동일한 모델에서 프롬프트 하나의 차이로 Structure F1이 0%에서 79%로 변했다는 사실은,
            소형 모델 활용 시 프롬프트 엔지니어링이 모델 스케일업의 대안이 될 수 있음을 시사합니다.
          </p>
        </blockquote>

        <h3>3.5 측정 지표 정의</h3>

        <h4>Character Error Rate (CER)</h4>
        <p>문자 수준의 정확도를 Levenshtein 편집 거리로 측정합니다:</p>
        <p class="report-content__caption">
          CER = (Substitutions + Deletions + Insertions) / Reference Length
        </p>

        <h4>Word Error Rate (WER)</h4>
        <p>단어 수준의 정확도를 측정합니다:</p>
        <p class="report-content__caption">
          WER = (Word Substitutions + Word Deletions + Word Insertions) / Reference Word Count
        </p>

        <h4>Structure F1</h4>
        <p>마크다운 구조 요소(Heading, List, Table) 검출 정확도입니다:</p>
        <pre><code class="language-text">평가 대상 구조 요소:
| Element Type   | Pattern           | Example              |
|----------------|-------------------|----------------------|
| Heading        | ^#{1,6}\s+        | # Title, ## Section  |
| Unordered List | ^[\s]*[-*+]\s+    | - item               |
| Ordered List   | ^[\s]*\d+\.\s+    | 1. first             |
| Table Row      | ^\|.+\|$          | | col1 | col2 |      |</code></pre>

        <h4>Boundary Clarity (BC)</h4>
        <p>
          MoC 논문<sup>[1]</sup>에 따르면, BC는 인접 청크 간 의미적 분리도를 측정합니다:
        </p>
        <p class="report-content__caption">
          BC = 1 - cosine_similarity(chunk<sub>i</sub>, chunk<sub>i+1</sub>)
        </p>
        <p>
          임베딩 공간에서 연속된 청크가 얼마나 다른지 측정합니다.
          <strong>BC Score가 높을수록</strong> 청킹 경계가 의미 단위를 잘 분리함을 의미합니다.
          값 범위는 0~1이며, 1에 가까울수록 경계가 명확합니다.
        </p>

        <h4>Chunk Stickiness (CS)</h4>
        <p>
          MoC 논문<sup>[1]</sup>에 따르면, CS는 청크 내 문장들의 결속도를 Structural Entropy로 측정합니다:
        </p>
        $$
        CS = -\sum_{i} \frac{h_i}{2m} \log_2 \frac{h_i}{2m}
        $$
        <p>
          여기서 h<sub>i</sub>는 문장 i가 다른 청크와 연결된 정도, m은 총 연결 수입니다.
          <strong>CS가 낮을수록</strong> 청크 내 문장들이 강하게 결합되어 있음을 의미합니다.
          값이 높으면 문장들이 여러 청크에 분산되어 정보가 파편화됨을 나타냅니다.
        </p>

        <!-- Experimental Setup -->
        <h2>4. Experimental Setup</h2>

        <h3>4.1 테스트 문서</h3>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>ID</th>
              <th>문서 유형</th>
              <th>언어</th>
              <th>특성</th>
              <th>PDF Type</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>test_1</td>
              <td>정부 공문서</td>
              <td>한국어</td>
              <td>스캔, 다단 레이아웃</td>
              <td><strong>Scanned PDF</strong></td>
            </tr>
            <tr>
              <td>test_2</td>
              <td>영수증</td>
              <td>영어</td>
              <td>이미지, 표 구조</td>
              <td><strong>N/A (이미지 파일)</strong></td>
            </tr>
            <tr>
              <td>test_3</td>
              <td>학술 논문</td>
              <td>영어</td>
              <td>2단 레이아웃, 표, 수식</td>
              <td><strong>Digital PDF</strong></td>
            </tr>
          </tbody>
        </table></div>

        <h4>4.1.1 커버리지 매트릭스</h4>
        <p>
          결과를 일반화하려면 다양한 PDF Type에서 검증이 필요합니다.
          현재 커버리지 현황은 다음과 같습니다:
        </p>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>PDF Type</th>
              <th>현재 커버리지</th>
              <th>비고</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Digital PDF</td>
              <td>✓ (test_3)</td>
              <td>텍스트 추출 정확도 높음</td>
            </tr>
            <tr>
              <td>Scanned PDF</td>
              <td>✓ (test_1)</td>
              <td>VLM Hallucination 위험 확인됨</td>
            </tr>
            <tr>
              <td>Hybrid PDF</td>
              <td>✗</td>
              <td>향후 확장 필요</td>
            </tr>
            <tr>
              <td>Image-heavy PDF</td>
              <td>△</td>
              <td>test_2는 순수 이미지, PDF 형태로 추가 테스트 필요</td>
            </tr>
          </tbody>
        </table></div>

        <h3>4.2 실험 환경</h3>
        <pre><code class="language-yaml"># Hardware Configuration
hardware:
  gpu: "NVIDIA RTX PRO 6000 Blackwell x2"
  vram: "96GB each (96GB x2)"
  ram: "128GB DDR5"
  storage: "NVMe SSD"

# Parser Configuration
vlm_parser:
  model: "Qwen3-VL-2B-Instruct"
  temperature: 0.1  # 일관된 출력
  max_tokens: 8192  # 긴 문서 지원
  image_resolution: 300 DPI

# Chunking Configuration (Controlled Variable)
chunking:
  strategy: "semantic"
  chunker: "LangChain SemanticChunker"
  breakpoint_threshold_type: "percentile"
  breakpoint_threshold_amount: 95.0
  min_chunk_size: null

# Embedding Configuration
embedding:
  chunking_model: "BAAI/bge-m3"                    # SemanticChunker 경계 검출용
  metrics_model: "jhgan/ko-sroberta-multitask"      # BC/CS 평가용

# Normalization
normalization:
  # CER/WER 측정 시 마크다운 구문은 제거 후 비교
  remove_markdown: true
  lowercase: false
  strip_whitespace: true</code></pre>

        <h3>4.3 A/B 실험 설계</h3>
        <p>
          <strong>통제 변수</strong>: 청킹 알고리즘(SemanticChunker), breakpoint threshold, 임베딩 모델, 검색 방식을 동일하게 유지하고,
          <strong>독립 변수</strong>(파서 유형)만 변경하여 파싱 품질의 영향을 분리 측정했습니다.
        </p>

        <h3>4.4 실제 파싱 결과 비교</h3>
        <p>동일한 학술 논문(Attention Is All You Need)의 테이블을 각 파서로 추출한 결과를 살펴보겠습니다:</p>

        <h4>Baseline (PyMuPDF) 출력</h4>
        <pre><code class="language-text">Layer Type Self-Attention (encoder)
Self-Attention (decoder) Encoder-Decoder Attention
Feed-Forward Complexity per Layer O(n2 · d) O(n2 · d)
Sequential Operations O(1) Minimum Path Length O(1)</code></pre>
        <p class="value-negative--note">
          ⚠️ 테이블 구조가 완전히 붕괴되어 텍스트 스트림으로 변환됨
        </p>

        <h4>Advanced (VLM) 출력</h4>
        <pre><code class="language-markdown">| Layer Type | Complexity per Layer | Sequential Operations | Minimum Path Length |
|------------|---------------------|----------------------|---------------------|
| Self-Attention (encoder) | O(n² · d) | O(1) | O(1) |
| Self-Attention (decoder) | O(n² · d) | O(1) | O(1) |
| Encoder-Decoder Attention | O(n · m · d) | O(1) | O(1) |
| Feed-Forward | O(n · d²) | O(1) | O(1) |</code></pre>
        <p class="value-positive--note">
          ✓ 마크다운 테이블로 구조가 보존되어 청킹 시 테이블이 atomic unit으로 유지됨
        </p>

        <!-- Results -->
        <h2>5. Results</h2>

        <h3>5.1 RQ1: 전제 조건 검증 (CER/WER)</h3>
        <p>
          VLM 기반 파싱이 다운스트림에서 유의미하려면, 먼저 <strong>텍스트 정확도가 심각하게 훼손되지 않아야</strong> 합니다.
          이 전제를 검증하기 위해 CER(Character Error Rate)과 WER(Word Error Rate)을 측정했습니다.
        </p>

        <figure>
          <img src="images/fig2_cer_comparison.png" alt="CER Comparison" loading="lazy">
          <figcaption>Figure 1: 파서별 CER 비교. Image-Baseline이 가장 낮은 CER(40.79%)을 달성하지만 구조 정보가 없음</figcaption>
        </figure>

        <h4>CER (Character Error Rate)</h4>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Document</th>
              <th>Text-Baseline</th>
              <th>Image-Baseline</th>
              <th>Text-Advanced</th>
              <th>Image-Advanced</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>test_1 (Korean/Scanned)</td>
              <td>N/A</td>
              <td>91.87%</td>
              <td>N/A</td>
              <td class="value-negative"><strong>536.50%</strong></td>
            </tr>
            <tr>
              <td>test_2 (영수증 이미지)</td>
              <td>99.59%</td>
              <td>40.80%</td>
              <td>120.54%</td>
              <td class="value-positive"><strong>33.09%</strong></td>
            </tr>
            <tr>
              <td>test_3 (English/Digital)</td>
              <td>51.25%</td>
              <td class="value-positive"><strong>40.79%</strong></td>
              <td>64.11%</td>
              <td>57.71%</td>
            </tr>
          </tbody>
        </table></div>

        <h4>WER (Word Error Rate)</h4>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Document</th>
              <th>Text-Baseline</th>
              <th>Image-Baseline</th>
              <th>Text-Advanced</th>
              <th>Image-Advanced</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>test_1 (Korean/Scanned)</td>
              <td>N/A</td>
              <td>98.21%</td>
              <td>N/A</td>
              <td class="value-negative"><strong>421.43%</strong></td>
            </tr>
            <tr>
              <td>test_2 (영수증 이미지)</td>
              <td>100.00%</td>
              <td>52.17%</td>
              <td>115.22%</td>
              <td class="value-positive"><strong>43.48%</strong></td>
            </tr>
            <tr>
              <td>test_3 (English/Digital)</td>
              <td>62.89%</td>
              <td class="value-positive"><strong>51.55%</strong></td>
              <td>75.26%</td>
              <td>68.04%</td>
            </tr>
          </tbody>
        </table></div>

        <h4>전제 조건 판정</h4>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>문서 유형</th>
              <th>판정</th>
              <th>근거</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>영어 디지털 PDF</td>
              <td class="value-positive"><strong>Pass</strong></td>
              <td>CER 57.71%, 구조 보존 가치 &gt; 정확도 손실</td>
            </tr>
            <tr>
              <td>영어 스캔 PDF</td>
              <td class="value-positive"><strong>Pass</strong></td>
              <td>CER 33.09%로 오히려 개선</td>
            </tr>
            <tr>
              <td>한글 스캔 PDF</td>
              <td class="value-negative"><strong>Fail</strong></td>
              <td>CER 536% — Hallucination으로 인해 사용 불가</td>
            </tr>
          </tbody>
        </table></div>

        <h3>5.2 RQ2: 구조 보존 효과 (Structure F1)</h3>
        <figure>
          <img src="images/fig1_structure_f1_comparison.png" alt="Structure F1 Comparison" loading="lazy">
          <figcaption>Figure 2: 파서별 Structure F1 비교. Baseline 파서는 구조 검출 불가(0%), Advanced 파서는 최대 79.25% 달성</figcaption>
        </figure>

        <p>
          Baseline 파서들의 Structure F1은 <strong>모두 0%</strong>인 반면,
          VLM 기반 Advanced 파서는 <strong>79.25%</strong>를 달성했습니다.
        </p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Parser</th>
              <th>Structure F1</th>
              <th>Precision</th>
              <th>Recall</th>
              <th>TP</th>
              <th>FP</th>
              <th>FN</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Text-Baseline</td>
              <td>0.00%</td>
              <td>0.00%</td>
              <td>0.00%</td>
              <td>0</td>
              <td>11</td>
              <td>24</td>
            </tr>
            <tr>
              <td>Image-Baseline</td>
              <td>0.00%</td>
              <td>0.00%</td>
              <td>0.00%</td>
              <td>0</td>
              <td>0</td>
              <td>24</td>
            </tr>
            <tr>
              <td><strong>Text-Advanced</strong></td>
              <td><strong>79.25%</strong></td>
              <td>72.41%</td>
              <td>87.50%</td>
              <td>21</td>
              <td>8</td>
              <td>3</td>
            </tr>
            <tr>
              <td>Image-Advanced</td>
              <td>77.78%</td>
              <td>70.00%</td>
              <td>87.50%</td>
              <td>21</td>
              <td>9</td>
              <td>3</td>
            </tr>
          </tbody>
        </table></div>

        <p><strong>해석</strong>:</p>
        <ul>
          <li><strong>Recall 87.5%</strong>: Ground Truth의 24개 구조 요소 중 21개를 검출</li>
          <li><strong>Precision ~72%</strong>: 검출한 요소 중 약 72%가 정확 (일부 과검출)</li>
          <li><strong>FN 3개</strong>: 놓친 구조 요소 - 주로 세부 섹션 헤딩이나 중첩 구조</li>
        </ul>

        <figure>
          <img src="images/fig5_precision_recall.png" alt="Precision vs Recall" loading="lazy">
          <figcaption>Figure 3: 파서별 Precision-Recall 비교. Advanced 파서는 높은 Recall(87.5%)을 달성하며, Precision도 70% 이상 유지</figcaption>
        </figure>

        <h3>5.3 RQ3: 다운스트림 효과 (BC/CS)</h3>
        <p>
          구조가 보존된 파싱 결과가 실제 <strong>청킹 품질</strong>에 미치는 영향을 측정하기 위해
          MoC 논문<sup>[1]</sup>의 Boundary Clarity(BC)와 Chunk Stickiness(CS) 메트릭을 사용했습니다.
        </p>

        <h4>Boundary Clarity (BC)</h4>
        <p>
          인접 청크 간 코사인 비유사도로, 청킹 경계가 의미 단위를 잘 분리하는지 측정합니다.
          <strong>높을수록 좋습니다</strong>.
        </p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Parser</th>
              <th>BC Score</th>
              <th>청크 수</th>
              <th>해석</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Baseline (PyMuPDF)</td>
              <td>N/A</td>
              <td>-</td>
              <td>측정 불가 (구조 없음)</td>
            </tr>
            <tr>
              <td><strong>Advanced (VLM)</strong></td>
              <td><strong>0.512</strong></td>
              <td>35</td>
              <td>인접 청크 평균 51.2% 비유사</td>
            </tr>
          </tbody>
        </table></div>

        <blockquote>
          <p>
            <strong>해석</strong>: BC Score 0.512는 연속된 청크 쌍의 평균 코사인 비유사도가 51.2%임을 의미합니다.
            즉, 청킹 경계에서 의미가 명확히 전환되어 각 청크가 독립적인 의미 단위로 분리되고 있습니다.
            Baseline 파서는 구조 정보가 없어 BC Score를 측정할 수 없습니다.
          </p>
        </blockquote>

        <h4>Chunk Stickiness (CS)</h4>
        <p>
          청크 내 문장들의 결속도를 Structural Entropy로 측정합니다.
          <strong>낮을수록 좋습니다</strong> (청크 내 문장들이 강하게 결합됨).
        </p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Parser</th>
              <th>CS Score</th>
              <th>해석</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Baseline (PyMuPDF)</td>
              <td>N/A</td>
              <td>측정 불가 (구조 없음)</td>
            </tr>
            <tr>
              <td><strong>Advanced (VLM)</strong></td>
              <td><strong>2.847</strong></td>
              <td>문장 결속도 양호</td>
            </tr>
          </tbody>
        </table></div>

        <blockquote>
          <p>
            <strong>해석</strong>: CS Score 2.847은 청크 내 문장들이 비교적 강하게 결합되어 있음을 나타냅니다.
            Structural Entropy가 낮을수록 정보 파편화가 적어 검색 시 관련 정보가 함께 반환될 가능성이 높습니다.
          </p>
        </blockquote>

        <h3>5.4 트레이드오프 시각화</h3>
        <figure>
          <img src="images/fig3_tradeoff_scatter.png" alt="Trade-off Analysis" loading="lazy">
          <figcaption>Figure 4: 텍스트 정확도(CER) vs 구조화 품질(Structure F1) 트레이드오프. 이상적인 지점은 우상단이지만, 현실적으로 Baseline과 Advanced 중 선택이 필요</figcaption>
        </figure>

        <pre><code class="language-text">                    텍스트 정확도 (CER ↓)
                           ▲
                           │
         Baseline          │         (이상적)
         ┌─────┐           │
         │ 좋음 │           │
         └─────┘           │
                           │
    ─────────────────────────────────────▶ 구조화 품질 (F1 ↑)
                           │
                           │         Advanced
                           │         ┌─────┐
                           │         │ 좋음 │
                           │         └─────┘</code></pre>

        <h3>5.5 Latency 분석</h3>
        <figure>
          <img src="images/fig4_latency_breakdown.png" alt="Latency Breakdown" loading="lazy">
          <figcaption>Figure 5: 처리 시간 분석. VLM 구조화 단계(Stage 2)가 전체 시간의 90% 이상을 차지</figcaption>
        </figure>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Parser</th>
              <th>Stage 1</th>
              <th>Stage 2 (VLM)</th>
              <th>Total</th>
              <th>배수</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Image-Baseline</td>
              <td>0.27s</td>
              <td>-</td>
              <td><strong>0.27s</strong></td>
              <td>1x</td>
            </tr>
            <tr>
              <td>Text-Baseline</td>
              <td>2.31s</td>
              <td>-</td>
              <td>2.31s</td>
              <td>8.6x</td>
            </tr>
            <tr>
              <td>Image-Advanced</td>
              <td>0.27s</td>
              <td>35.48s</td>
              <td>35.75s</td>
              <td>132x</td>
            </tr>
            <tr>
              <td>Text-Advanced</td>
              <td>2.28s</td>
              <td>40.64s</td>
              <td><strong>42.92s</strong></td>
              <td>159x</td>
            </tr>
          </tbody>
        </table></div>

        <p>
          <strong>159배</strong>의 latency 증가는 부담스럽지만,
          <strong>Structure F1 +79%p</strong> 개선과 <strong>BC Score 0.512</strong>가 RAG 품질에 미치는 영향을 고려하면
          구조가 중요한 문서에서는 충분히 정당화될 수 있습니다.
        </p>

        <h3>5.6 RQ4: Retrieval 영향 평가 (진행 중)</h3>
        <p>
          RQ4는 개선된 청킹 품질이 실제 검색 정밀도(Hit Rate@k, MRR)를 높이는지 검증하는 End-to-End 평가입니다.
          RQ1-RQ3에서 확인한 구조 보존 → 청킹 품질 개선의 효과가
          최종 검색 단계까지 전파되는지를 직접 측정합니다.
        </p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>지표</th>
              <th>정의</th>
              <th>측정 방식</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>Hit Rate@k</strong></td>
              <td>상위 k개 검색 결과에 정답 청크가 포함되는 비율</td>
              <td>k = {1, 3, 5, 10}</td>
            </tr>
            <tr>
              <td><strong>MRR</strong></td>
              <td>첫 번째 정답 청크의 역순위 평균 (Mean Reciprocal Rank)</td>
              <td>$MRR = \frac{1}{|Q|}\sum_{i=1}^{|Q|}\frac{1}{rank_i}$</td>
            </tr>
          </tbody>
        </table></div>

        <blockquote>
          <p>
            <strong>현재 상태</strong>: RQ1-RQ3의 결과는 구조 보존이 청킹 품질까지 개선한다는 것을 보여줍니다.
            그러나 이것이 실제 검색 정밀도 향상으로 이어지는지는 아직 직접 측정하지 못했습니다.
            BC Score 0.512와 CS Score 2.847은 긍정적인 <em>간접 증거</em>이지만,
            Hit Rate@k와 MRR을 통한 직접 검증이 필요합니다.
            2026년 2월 내 실험 완료를 목표로 하고 있습니다.
          </p>
        </blockquote>

        <!-- Error Analysis -->
        <h2>6. Error Analysis</h2>

        <h3>6.1 VLM Error Categories</h3>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Category</th>
              <th>빈도</th>
              <th>심각도</th>
              <th>Root Cause</th>
              <th>예시</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="value-negative"><strong>HALLUCINATION</strong></td>
              <td>High (test_1)</td>
              <td>Critical</td>
              <td>불명확한 내용 과해석</td>
              <td>CER 536% (한글 스캔)</td>
            </tr>
            <tr>
              <td>FALSE_POSITIVE_STRUCTURE</td>
              <td>27-30%</td>
              <td>Medium</td>
              <td>구조 과검출</td>
              <td>FP=8-9 in test_3</td>
            </tr>
            <tr>
              <td>MISSED_STRUCTURE</td>
              <td>12.5%</td>
              <td>Medium</td>
              <td>미묘한 포맷 누락</td>
              <td>FN=3 in test_3</td>
            </tr>
            <tr>
              <td>LATENCY</td>
              <td>100%</td>
              <td>Varies</td>
              <td>VLM 추론 시간</td>
              <td>42.92s vs 0.27s</td>
            </tr>
          </tbody>
        </table></div>

        <h3>6.2 Hallucination 상세 분석 — 한글 스캔 문서는 왜 실패하는가</h3>
        <p>
          test_1(한글 스캔 문서)에서 VLM이 <strong>CER 536%</strong>를 기록한 것은
          원본 텍스트(522자)보다 <strong>36배 긴 텍스트(19,033자)</strong>를 생성했음을 의미합니다.
          동일한 프롬프트(v2)와 파이프라인으로 영어 스캔 문서(test_2)는 CER 33%로 정상 작동한 반면,
          한글에서만 이러한 극단적 실패가 발생한 원인은 세 가지 요인의 복합 작용입니다.
        </p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>문서</th>
              <th>언어</th>
              <th>CER (Image-Advanced)</th>
              <th>원본 글자수</th>
              <th>VLM 출력 글자수</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>test_1</td>
              <td><strong>한국어</strong></td>
              <td class="value-negative"><strong>536.50%</strong></td>
              <td>522자</td>
              <td><strong>19,033자</strong> (36x)</td>
            </tr>
            <tr>
              <td>test_2</td>
              <td>영어</td>
              <td class="value-positive">33.09%</td>
              <td>-</td>
              <td>정상</td>
            </tr>
            <tr>
              <td>test_3</td>
              <td>영어</td>
              <td class="value-positive">57.71%</td>
              <td>-</td>
              <td>정상</td>
            </tr>
          </tbody>
        </table></div>

        <h4>원인 1: VLM 학습 데이터 편향 (근본 원인)</h4>
        <p>
          Qwen3-VL-2B는 주로 <strong>영어와 중국어</strong> 데이터로 학습되었으며,
          한국어 문서 데이터의 학습 비중은 현저히 낮습니다.
          모델이 한국어 글자를 "확신 있게 읽지 못하는" 상태에서
          두 가지 행동 중 하나를 선택해야 합니다:
        </p>
        <ul>
          <li>프롬프트 지시대로 <code>[unclear]</code>로 표기하거나</li>
          <li><strong>문맥에서 추측하여 채워넣거나</strong> (실제 발생한 행동)</li>
        </ul>
        <p>
          2B 파라미터의 소형 모델은 프롬프트의 anti-hallucination 지시
          ("Do NOT add information that is not visible", "indicate with [unclear] rather than guessing")를
          충실히 따를 역량이 부족하여 후자로 빠진 것입니다.
        </p>

        <h4>원인 2: OCR 입력 품질의 극단적 저하</h4>
        <p>
          Image-Baseline의 한글 CER이 이미 <strong>91.87%</strong>입니다.
          즉 OCR 단계에서 거의 모든 글자가 틀린 채로 VLM에 전달됩니다.
          영어 Baseline CER은 40% 수준으로 VLM이 충분히 복원할 수 있었지만,
          한글의 92% 에러율은 <strong>복원 자체가 불가능한 수준</strong>입니다.
          VLM은 "거의 읽을 수 없는" 입력을 구조화하라는 지시를 받고,
          원본을 알 수 없으니 <em>추론 → 없는 내용 생성</em>으로 이어진 것입니다.
        </p>

        <h4>원인 3: 한국어 처리 파이프라인 미최적화</h4>
        <p>
          한국어 WER 측정 시 <strong>MeCab(형태소 분석기)을 적용하지 않고</strong>
          단순 공백 기반 토크나이징을 사용했습니다.
          이는 직접적 hallucination 원인은 아니지만,
          파이프라인 전체가 한국어에 최적화되지 않았음을 보여주는 증거입니다.
        </p>

        <h4>구조 검출 결과: 완전한 실패</h4>
        <p>
          세 가지 원인이 복합적으로 작용한 결과,
          test_1의 구조 검출은 다음과 같이 완전히 실패했습니다:
        </p>

        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>지표</th>
              <th>test_1 (한글)</th>
              <th>해석</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>True Positives</td>
              <td><strong>0</strong></td>
              <td>실제 구조를 하나도 맞추지 못함</td>
            </tr>
            <tr>
              <td>False Positives</td>
              <td class="value-negative"><strong>831</strong></td>
              <td>존재하지 않는 헤딩·리스트·테이블을 생성</td>
            </tr>
            <tr>
              <td>False Negatives</td>
              <td>19</td>
              <td>실제 구조 19개를 모두 놓침</td>
            </tr>
            <tr>
              <td>Structure F1</td>
              <td><strong>0.00%</strong></td>
              <td>831개의 구조를 만들었으나 전부 가짜</td>
            </tr>
          </tbody>
        </table></div>

        <blockquote>
          <p>
            <strong>결론</strong>: <em>낮은 OCR 품질(92% 에러) + 한국어 학습 데이터 부족 + 소형 모델(2B)의 지시 따르기 한계</em>가 결합되어,
            모델이 "읽을 수 없는 한글"을 "추측으로 채우는" 모드로 전환한 것이 근본 원인입니다.
            프롬프트 v2의 anti-hallucination 규칙은 영어에서는 효과적이었으나,
            한글에서는 모델의 언어 이해 능력 자체가 부족하여 무력화되었습니다.
            품질이 낮은 한글 스캔에는 VLM 구조화를 적용하지 않고 Image-Baseline을 사용하는 것이 안전합니다.
          </p>
        </blockquote>

        <h3>6.3 Traditional OCR Error Categories</h3>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Category</th>
              <th>빈도</th>
              <th>심각도</th>
              <th>Root Cause</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td class="value-negative"><strong>STRUCTURE_LOSS</strong></td>
              <td>100%</td>
              <td>Critical</td>
              <td>레이아웃 이해 불가</td>
            </tr>
            <tr>
              <td>TABLE_COLLAPSE</td>
              <td>100%</td>
              <td>Critical</td>
              <td>테이블이 텍스트 스트림으로 변환</td>
            </tr>
            <tr>
              <td>COLUMN_MIX</td>
              <td>High</td>
              <td>Critical</td>
              <td>다단 레이아웃 읽기 순서 오류</td>
            </tr>
          </tbody>
        </table></div>

        <!-- Discussion -->
        <h2>7. Discussion</h2>

        <h3>7.1 Research Questions 답변</h3>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>RQ</th>
              <th>역할</th>
              <th>답변</th>
              <th>근거</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>RQ1</strong>: VLM이 텍스트 정확도를 유지하는가?</td>
              <td>전제조건</td>
              <td class="value-warning"><strong>부분적</strong></td>
              <td>영어 스캔 문서에서 CER 개선 (40.8% → 33.1%), 단 Hallucination 위험 존재</td>
            </tr>
            <tr>
              <td><strong>RQ2</strong>: VLM이 구조 보존에 효과적인가?</td>
              <td>핵심 가설</td>
              <td class="value-positive"><strong>Yes</strong></td>
              <td>Structure F1: 0% → 79.25% (+79pp)</td>
            </tr>
            <tr>
              <td><strong>RQ3</strong>: 구조 보존이 다운스트림 품질을 향상시키는가?</td>
              <td>효과 검증</td>
              <td class="value-positive"><strong>Yes</strong></td>
              <td>BC 0.512 (높음=좋음), CS 2.847 (낮음=좋음)</td>
            </tr>
            <tr>
              <td><strong>RQ4</strong>: 개선된 청킹이 실제 검색 정밀도를 높이는가?</td>
              <td>End-to-End</td>
              <td class="value-neutral"><strong>진행 중</strong></td>
              <td>Hit Rate@k, MRR 직접 측정 예정 (간접 증거: BC/CS에서 긍정적)</td>
            </tr>
          </tbody>
        </table></div>

        <h3>7.2 Hybrid 파싱 전략 제안</h3>
        <p>실험 결과를 바탕으로 다음과 같은 문서 라우팅 전략을 제안합니다:</p>

        <pre><code class="language-text">                    Document Input
                         │
                         ▼
                    ┌─────────┐
                    │ Scanned?│──────Yes────► VLM (Required)
                    └────┬────┘               ⚠️ 한글은 주의
                         │No
                         ▼
                    ┌──────────────┐
                    │ Complex      │
                    │ Layout?      │──────Yes────► VLM (Recommended)
                    │ (Tables,     │
                    │ Multi-column)│
                    └──────┬───────┘
                           │No
                           ▼
                    PyMuPDF (Fast, Sufficient)</code></pre>

        <h3>7.3 Parser Selection Guide</h3>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>사용 목적</th>
              <th>권장 파서</th>
              <th>이유</th>
              <th>예상 성능</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>텍스트 검색/인덱싱</td>
              <td>Baseline</td>
              <td>높은 텍스트 정확도</td>
              <td>CER ~41%, Latency ~0.3s</td>
            </tr>
            <tr>
              <td>RAG/Chunking</td>
              <td>Advanced</td>
              <td>구조 기반 청킹 가능</td>
              <td>F1 ~79%, Latency ~43s</td>
            </tr>
            <tr>
              <td>문서 변환 (HTML/MD)</td>
              <td>Advanced</td>
              <td>마크다운 구조 활용</td>
              <td>-</td>
            </tr>
            <tr>
              <td>실시간 처리</td>
              <td>Baseline</td>
              <td>낮은 Latency</td>
              <td>0.27s</td>
            </tr>
          </tbody>
        </table></div>

        <!-- Limitations -->
        <h2>8. Limitations</h2>

        <h3>8.1 Dataset Limitations</h3>

        <h4>8.1.1 통계적 유의성</h4>
        <ul>
          <li>
            <strong>샘플 크기</strong>: 3개 문서로는 p-value 계산 자체가 무의미합니다.
            중심극한정리(CLT)에 따르면 n≥30에서 표본 평균이 정규분포에 근사하므로,
            통계적 검정을 위해서는 최소 30개 이상의 샘플이 필요합니다.
          </li>
          <li>
            <strong>효과 크기 관점</strong>: Structure F1이 0% → 79%로 효과 크기(effect size)가 매우 크므로,
            <em>방향성</em>은 신뢰할 수 있습니다. 다만 "79.25%"라는 정확한 수치는 추가 검증이 필요합니다.
          </li>
          <li>
            <strong>해석 범위</strong>: 현재 결과는 "VLM이 구조 보존에 효과적이다"는 가설을 지지하는
            <em>예비 증거(preliminary evidence)</em>로 해석해야 합니다.
          </li>
        </ul>

        <h4>8.1.2 PDF Type 커버리지</h4>
        <ul>
          <li><strong>Hybrid PDF 미포함</strong>: 일부 페이지만 스캔된 문서 유형이 테스트되지 않았습니다.</li>
          <li><strong>Image-heavy PDF 제한적</strong>: 표/차트가 이미지로 삽입된 Digital PDF 추가가 필요합니다.</li>
          <li><strong>문서 다양성</strong>: 한국어/영어만 포함되어 있고, 금융/법률 도메인은 미포함입니다.</li>
          <li><strong>Ground Truth 품질</strong>: 단일 주석자가 작성했으며, 마크다운 스타일 선택이 지표에 영향을 줄 수 있습니다.</li>
        </ul>

        <h3>8.2 Methodological Limitations</h3>
        <ul>
          <li><strong>단일 VLM 모델</strong>: Qwen3-VL 결과가 다른 VLM에 일반화되지 않을 수 있습니다.</li>
          <li><strong>고정 청킹 파라미터</strong>: 최적 설정이 아닐 수 있습니다.</li>
          <li><strong>End-to-End 검증 미완</strong>: Hit Rate@k 직접 측정은 향후 과제로 남아 있습니다.</li>
        </ul>

        <!-- Future Work -->
        <h2>9. Future Work</h2>

        <h3>9.1 단기 과제 (3-6개월)</h3>

        <h4>PDF Type 다양화 로드맵</h4>
        <p>
          PDF Type 다양화를 파일 포맷 다양화(DOCX, HWP)보다 우선합니다.
          파서가 다르게 동작하는 조건을 먼저 커버해야 결과를 일반화할 수 있기 때문입니다.
        </p>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>단계</th>
              <th>목표 샘플</th>
              <th>구성</th>
              <th>목적</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><strong>1단계</strong></td>
              <td>10-15개</td>
              <td>
                Digital PDF × 3-5<br>
                Scanned PDF × 3-5<br>
                Hybrid PDF × 2-3<br>
                Image-heavy PDF × 2-3
              </td>
              <td>
                PDF Type별 일관된 패턴 확인<br>
                (통계 검정은 불가하지만 트렌드 파악 가능)
              </td>
            </tr>
            <tr>
              <td><strong>2단계</strong></td>
              <td>30-50개</td>
              <td>층화 샘플링 (Type × 도메인)</td>
              <td>Paired t-test 가능, 고객사 제안/블로그 신뢰성</td>
            </tr>
            <tr>
              <td><strong>3단계</strong></td>
              <td>50-100개</td>
              <td>벤치마크 수준</td>
              <td>학술 논문 제출 가능, p-value 계산 의미 있음</td>
            </tr>
          </tbody>
        </table></div>

        <ul>
          <li><strong>End-to-End RAG 평가</strong>: LLM 답변 생성까지 연결하여 RAGAs 메트릭(Faithfulness, Answer Relevancy)으로 평가</li>
          <li><strong>Ablation Study 완료</strong>: 프롬프트 변형, 해상도 최적화, 임베딩 모델 비교</li>
        </ul>

        <h3>9.2 중기 과제 (6-12개월)</h3>
        <ul>
          <li><strong>Hybrid System 구현</strong>: 문서 자동 분류기 + 파서 선택 로직 + 프로덕션 파이프라인</li>
          <li><strong>다국어 확장</strong>: 중국어, 일본어 문서 지원 및 교차 언어 검색 평가</li>
          <li><strong>효율성 연구</strong>: 작은 VLM 모델(distillation), 배치 처리 최적화</li>
        </ul>

        <h3>9.3 장기 과제 (1년+)</h3>
        <ul>
          <li><strong>Adaptive Parsing</strong>: 문서 유형별 최적 파서 자동 학습 (강화학습 기반)</li>
          <li><strong>벤치마크 공개</strong>: 공개 데이터셋 + 리더보드 + 커뮤니티 평가 표준</li>
        </ul>

        <!-- Conclusion -->
        <h2>10. Conclusion</h2>

        <h3>10.1 기여</h3>
        <ol>
          <li><strong>Multi-Metric 평가 프레임워크</strong>: Lexical, Structural, Downstream, Retrieval 4차원 평가 체계 제시</li>
          <li><strong>트레이드오프 정량화</strong>: Structure F1 +79pp vs CER +17pp vs Latency 159x</li>
          <li><strong>Prompt Engineering 방법론</strong>: Role Framing, Explicit Negation, Uncertainty Handling 등 5가지 전략으로 Structure F1을 0% → 79.25%로 개선</li>
          <li><strong>다운스트림 효과 검증</strong>: BC 0.512, CS 2.847로 구조 보존이 청킹 품질에 미치는 영향 확인</li>
          <li><strong>Hybrid 전략 제안</strong>: 문서 특성별 최적 파서 라우팅 가이드</li>
          <li><strong>Hallucination 경고</strong>: 한글 스캔 문서에서 VLM 적용 시 주의사항 문서화</li>
        </ol>

        <h3>10.2 정량화된 트레이드오프 (test_3 기준)</h3>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>지표</th>
              <th>Baseline</th>
              <th>Advanced</th>
              <th>변화</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Structure F1</td>
              <td>0%</td>
              <td>79.25%</td>
              <td class="value-positive"><strong>+79pp</strong></td>
            </tr>
            <tr>
              <td>BC (Boundary Clarity)</td>
              <td>N/A</td>
              <td>0.512</td>
              <td class="value-positive"><strong>높을수록 좋음</strong></td>
            </tr>
            <tr>
              <td>CS (Chunk Stickiness)</td>
              <td>N/A</td>
              <td>2.847</td>
              <td class="value-positive"><strong>낮을수록 좋음</strong></td>
            </tr>
            <tr>
              <td>CER</td>
              <td>40.79%</td>
              <td>57.71%</td>
              <td class="value-negative">+17pp</td>
            </tr>
            <tr>
              <td>Latency</td>
              <td>0.27s</td>
              <td>42.92s</td>
              <td class="value-negative">159x</td>
            </tr>
          </tbody>
        </table></div>

        <h3>10.3 Closing Remarks</h3>
        <blockquote>
          <p>
            "구조 보존은 RAG 시스템에서 중요합니다.
            VLM 기반 파싱은 만능 해결책이 아니지만, 복잡한 문서에서는 확실히 도움이 됩니다.
            <strong>파싱 품질에 투자하면 하류(downstream) 검색 정확도에서 보상받습니다.</strong>"
          </p>
        </blockquote>

        <hr>

        <h2>References</h2>

        <h3>Academic Papers</h3>
        <ul>
          <li>Xu et al. (2020). <em>LayoutLM: Pre-training of Text and Layout for Document Image Understanding</em>. ACL 2020.</li>
          <li>Xu et al. (2022). <em>LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking</em>. ACM MM 2022.</li>
          <li>Bai et al. (2023). <em>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</em>. arXiv:2308.12966.</li>
          <li>Wang et al. (2024). <em>Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution</em>. arXiv:2409.12191.</li>
          <li>Lewis et al. (2020). <em>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</em>. NeurIPS 2020.</li>
          <li><strong>[1] Zhao et al. (2025). <em>MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation</em>. arXiv:2503.09600.</strong> (BC/CS 메트릭 출처)</li>
          <li><strong>[2] Van Rijsbergen, C. J. (1979). <em>Information Retrieval</em> (2nd ed.). Butterworths.</strong> (F-measure / Structure F1 공식 출처)</li>
          <li>Vaswani et al. (2017). <em>Attention Is All You Need</em>. NeurIPS 2017. (test document)</li>
          <li>Wei et al. (2022). <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em>. NeurIPS 2022. (test document)</li>
        </ul>

        <h3>Software Tools</h3>
        <div class="table-wrap"><table>
          <thead>
            <tr>
              <th>Tool</th>
              <th>Version</th>
              <th>Purpose</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>PyMuPDF (fitz)</td>
              <td>1.24.x</td>
              <td>Digital PDF text extraction</td>
            </tr>
            <tr>
              <td>RapidOCR</td>
              <td>1.3.x</td>
              <td>Image-based OCR</td>
            </tr>
            <tr>
              <td>Qwen3-VL-2B-Instruct</td>
              <td>-</td>
              <td>Vision-Language Model for structured parsing</td>
            </tr>
            <tr>
              <td>sentence-transformers</td>
              <td>2.2.x</td>
              <td>Embedding generation</td>
            </tr>
            <tr>
              <td>LangChain</td>
              <td>0.3.x</td>
              <td>Semantic chunking framework</td>
            </tr>
          </tbody>
        </table></div>

        <p class="report-content__callout">
          <strong>Keywords</strong>: Vision-Language Models, Document Parsing, RAG, Semantic Chunking, OCR, Qwen-VL, Hybrid Strategy, Boundary Clarity, Chunk Stickiness, MoC, Structure Preservation
        </p>

      </div>
    </article>
  </main>

  <!-- Table of Contents Minimap -->
  <nav class="report-toc" id="toc" aria-label="Table of Contents">
    <div class="report-toc__title">On this page</div>
    <ul class="report-toc__list" id="toc-list">
      <!-- Dynamically generated -->
    </ul>
  </nav>

  <footer class="report-footer">
    <p class="report-footer__copy">&copy; 2026 Hyeongseob Kim</p>
  </footer>

  <!-- KaTeX -->
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

  <!-- Prism.js -->
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>

  <script>
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false }
      ]
    });

    // Table of Contents Generator
    (function() {
      const tocList = document.getElementById('toc-list');
      const content = document.querySelector('.report-content');
      if (!tocList || !content) return;

      // Get all h2 headings in report-content
      const headings = content.querySelectorAll('h2');
      if (headings.length === 0) return;

      // Generate IDs and TOC items
      headings.forEach((heading, index) => {
        // Create ID if not exists
        if (!heading.id) {
          heading.id = 'section-' + index;
        }

        // Create TOC item
        const li = document.createElement('li');
        li.className = 'report-toc__item';

        const a = document.createElement('a');
        a.className = 'report-toc__link';
        a.href = '#' + heading.id;
        a.textContent = heading.textContent;

        // Smooth scroll on click
        a.addEventListener('click', function(e) {
          e.preventDefault();
          const target = document.getElementById(heading.id);
          if (target) {
            const headerOffset = 80;
            const elementPosition = target.getBoundingClientRect().top;
            const offsetPosition = elementPosition + window.pageYOffset - headerOffset;

            window.scrollTo({
              top: offsetPosition,
              behavior: 'smooth'
            });

            // Update URL hash without jumping
            history.pushState(null, null, '#' + heading.id);
          }
        });

        li.appendChild(a);
        tocList.appendChild(li);
      });

      // Scroll spy - highlight current section
      const tocLinks = tocList.querySelectorAll('.report-toc__link');

      function updateActiveLink() {
        const scrollPosition = window.scrollY + 100;

        let currentIndex = 0;
        headings.forEach((heading, index) => {
          if (heading.offsetTop <= scrollPosition) {
            currentIndex = index;
          }
        });

        tocLinks.forEach((link, index) => {
          if (index === currentIndex) {
            link.classList.add('report-toc__link--active');
          } else {
            link.classList.remove('report-toc__link--active');
          }
        });
      }

      // Throttle scroll event
      let ticking = false;
      window.addEventListener('scroll', function() {
        if (!ticking) {
          window.requestAnimationFrame(function() {
            updateActiveLink();
            ticking = false;
          });
          ticking = true;
        }
      });

      // Initial update
      updateActiveLink();
    })();
  </script>
</body>

</html>
